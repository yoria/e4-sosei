{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "tyukan.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/h30e07/E4sosei/blob/main/tyukan.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xMKWhTVjhbAO",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 413
        },
        "outputId": "15afab4b-2909-46f9-90b4-6b927fd6bf03"
      },
      "source": [
        "def read_url(url):\n",
        "  r = requests.get(url)\n",
        "  print(r.text)\n",
        "  soup = BeautifulSoup(r.text, \"lxml\")\n",
        "  h4_list = soup.find_all('h4')\n",
        "  for h4 in h4_list:\n",
        "    return h4.text\n",
        "\n",
        "def all_subject_kanrensei():\n",
        "  '''\n",
        "  全ての2ペアの科目の関連性を数値化\n",
        "  最大1\n",
        "  これより科目間を線で結ぶか結ばないかを判定\n",
        "  線を引くかどうかの基準の数値は自分で設定(いい感じになるように)\n",
        "  '''\n",
        "\n",
        "def kanren_word():\n",
        "  '''\n",
        "  関連していそうな単語全てを、ファクターに通して、重要度を数値化\n",
        "  最大1\n",
        "  ファクターどうしよう。。。\n",
        "  重要度が自分で設定した数値を超えていたら表示\n",
        "  '''\n",
        "\n",
        "def two_shirabasu_kanrendo():\n",
        "  from gensim.models.word2vec import Word2Vec\n",
        "\n",
        "  path = \"/content/drive/MyDrive/sosei/latest-ja-word2vec-gensim-model/word2vec.gensim.model\"\n",
        "  w2v_model = Word2Vec.load(path)\n",
        "\n",
        "  sa = \"hoge\"\n",
        "  sb = \"hoge\"\n",
        "  sa_words = sa.split()\n",
        "  sb_words = sb.split()\n",
        "  sum = 0\n",
        "  for sa_word in sa_words:\n",
        "    for sb_word in sb_words:\n",
        "      sum += model.similarity(sa_word, sb_word)\n",
        "  output = sum / (len(sa_words) + len(sb_words))\n",
        "\n",
        "  return output\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-2f26c033f7f8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m# corpus = gendl.load(\"text8\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWord2Vec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/model/word2vec.gensim.model'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;31m# 単語間の類似度計算\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msimilarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ステーキ\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"焼肉\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gensim/models/word2vec.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1310\u001b[0m         \"\"\"\n\u001b[1;32m   1311\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1312\u001b[0;31m             \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mWord2Vec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1313\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1314\u001b[0m             \u001b[0;31m# for backward compatibility for `max_final_vocab` feature\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gensim/models/base_any2vec.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1242\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1243\u001b[0m         \"\"\"\n\u001b[0;32m-> 1244\u001b[0;31m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBaseWordEmbeddingsModel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1245\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ns_exponent'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1246\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mns_exponent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.75\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gensim/models/base_any2vec.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(cls, fname_or_handle, **kwargs)\u001b[0m\n\u001b[1;32m    601\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    602\u001b[0m         \"\"\"\n\u001b[0;32m--> 603\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBaseAny2VecModel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname_or_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    604\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    605\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname_or_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gensim/utils.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(cls, fname, mmap)\u001b[0m\n\u001b[1;32m    421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    422\u001b[0m         \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munpickle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 423\u001b[0;31m         \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_load_specials\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmmap\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompress\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    424\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"loaded %s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    425\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gensim/utils.py\u001b[0m in \u001b[0;36m_load_specials\u001b[0;34m(self, fname, mmap, compress, subname)\u001b[0m\n\u001b[1;32m    451\u001b[0m             \u001b[0mcfname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'.'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrib\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    452\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"loading %s recursively from %s.* with mmap=%s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrib\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmmap\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 453\u001b[0;31m             \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrib\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_load_specials\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmmap\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompress\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    454\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    455\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mattrib\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'__numpys'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gensim/utils.py\u001b[0m in \u001b[0;36m_load_specials\u001b[0;34m(self, fname, mmap, compress, subname)\u001b[0m\n\u001b[1;32m    462\u001b[0m                 \u001b[0mval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubname\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrib\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    463\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 464\u001b[0;31m                 \u001b[0mval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubname\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrib\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmmap_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmmap\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    465\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    466\u001b[0m             \u001b[0msetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrib\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding)\u001b[0m\n\u001b[1;32m    414\u001b[0m             \u001b[0mown_fid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    415\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 416\u001b[0;31m             \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstack\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menter_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos_fspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    417\u001b[0m             \u001b[0mown_fid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    418\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/model/word2vec.gensim.model.wv.syn0.npy'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OcCpc7bWan8j"
      },
      "source": [
        "# **２つのシラバスの関連度を求める**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xp3G3AYlagZZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cab4d01f-d6f0-47c1-95c5-c3b3b2ea7211"
      },
      "source": [
        "from gensim.models.word2vec import Word2Vec\n",
        "import glob\n",
        "import pprint\n",
        "from google.colab import files\n",
        "import csv\n",
        "import time\n",
        "import json\n",
        "import itertools\n",
        "\n",
        "\n",
        "def create_csv(list2):\n",
        "  with open(output_csv_path, 'w', encoding=\"utf_8_sig\") as fw:\n",
        "    writer = csv.writer(fw)\n",
        "    writer.writerow(['s1', 's2', 'relevance'])\n",
        "    for one in list2:\n",
        "      writer.writerow(one)\n",
        "\n",
        "def tow_s_similarity(w2v_model, sa_path, sb_path):\n",
        "  '''\n",
        "  2つのシラバスの関連度を0から1の範囲で出力\n",
        "  作成済みword2vecモデルを使用\n",
        "  '''\n",
        "  with open(sa_path) as s1:\n",
        "    sa = s1.read()\n",
        "  with open(sb_path) as s2:\n",
        "    sb = s2.read()\n",
        "  sa_words = sa.split()\n",
        "  sb_words = sb.split()\n",
        "  sum = 0\n",
        "\n",
        "  bunpu = []\n",
        "  for sa_word in sa_words:\n",
        "    for sb_word in sb_words:\n",
        "      try:\n",
        "        similarity = w2v_model.similarity(sa_word, sb_word)\n",
        "        bunpu.append([float(similarity), sa_word, sb_word])\n",
        "        sum += similarity\n",
        "      except KeyError:\n",
        "        sum += 0\n",
        "  output = sum / (len(sa_words) * len(sb_words))\n",
        "  bunpu1 = sorted(bunpu, key=lambda x: x[0], reverse=True)\n",
        "  with open(f\"/content/drive/MyDrive/sosei/bunpu.json\", 'w') as f:\n",
        "    json.dump(bunpu1, f, indent=2, ensure_ascii=False)\n",
        "  return output\n",
        "\n",
        "def get_ignore_kumiawase(json):\n",
        "  ignore_kumiawase = []\n",
        "  for tt in json:\n",
        "    ignore_kumiawase.append((tt[0], tt[1]))\n",
        "  return ignore_kumiawase\n",
        "\n",
        "def get_jissai_kumiawase(all_kumiawase, ignore_kumiawase):\n",
        "  jissai_kumiawase = []\n",
        "  branch = \"ok\"\n",
        "  for ttt in all_kumiawase:\n",
        "    for ii in ignore_kumiawase:\n",
        "      branch = \"ok\"\n",
        "      if set(ttt) == set(ii):\n",
        "        branch = \"ignore\"\n",
        "        break\n",
        "    if branch == \"ok\":\n",
        "      jissai_kumiawase.append(ttt)\n",
        "  return jissai_kumiawase\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "  sosei_root = \"/content/drive/MyDrive/sosei\"\n",
        "  w2v_model_path = f\"{sosei_root}/Pre-Trained Word2Vec Models/latest-ja-word2vec-gensim-model/word2vec.gensim.model\"\n",
        "\n",
        "  w2v_model = Word2Vec.load(w2v_model_path)\n",
        "\n",
        "  files = glob.glob(f\"{sosei_root}/meishi/*\")\n",
        "  all_kumiawase = list(itertools.combinations(files,2))\n",
        "\n",
        "  output_csv_path = f\"{sosei_root}/seido1.csv\"\n",
        "  '''\n",
        "  json_open = open(f\"{sosei_root}/a.json\", 'r')\n",
        "  output = json.load(json_open)\n",
        "\n",
        "  ignore_kumiawase = get_ignore_kumiawase(output)\n",
        "  jissai_kumiawase = get_jissai_kumiawase(all_kumiawase, ignore_kumiawase)\n",
        "\n",
        "  print(len(jissai_kumiawase))\n",
        "  '''\n",
        "  tow_s_similarity(w2v_model, \"/content/drive/MyDrive/sosei/meishi/meishi_1基礎数学D(0077).txt\", \"/content/drive/MyDrive/sosei/meishi/meishi_3ディジタル回路ⅠA(2093).txt\")\n",
        "  '''\n",
        "  limit_minute = 30 #mm分でプログラムを終わらせて途中経過をファイルに保存\n",
        "  t1 = time.time()\n",
        "  for one in jissai_kumiawase:\n",
        "    relevance = tow_s_similarity(w2v_model, one[0], one[1])\n",
        "    output.append([one[0], one[1], relevance])\n",
        "\n",
        "    t2 = time.time()\n",
        "    elapsed_time = t2- t1\n",
        "    if elapsed_time > limit_minute * 60:\n",
        "      break\n",
        "\n",
        "  with open(f\"{sosei_root}/a.json\", 'w') as f:\n",
        "    json.dump(output, f, indent=2, ensure_ascii=False)\n",
        "  '''\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:35: DeprecationWarning: Call to deprecated `similarity` (Method will be removed in 4.0.0, use self.wv.similarity() instead).\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bggtltfvBFzI"
      },
      "source": [
        "出来上がった全科目の関連性を関連度で並び替え"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oXpqkKSDBSzO",
        "outputId": "814821c8-536b-459c-c11e-46a1166c96ad"
      },
      "source": [
        "import json\n",
        "json_path = \"/content/drive/MyDrive/sosei/a.json\"\n",
        "json_open = open(json_path, 'r')\n",
        "output = json.load(json_open)\n",
        "print(len(output))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "990\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MCrXtRKZbbi5"
      },
      "source": [
        "正解データのcsvファイル読み込みand科目の組み合わせのみをまとめたjsonファイルを出力"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EYW-VZWsbdwX",
        "outputId": "b1dbd663-8e05-4722-e825-80aeb7c693ba"
      },
      "source": [
        "import csv\n",
        "import pprint\n",
        "import json\n",
        "\n",
        "l = []\n",
        "with open('/content/drive/MyDrive/sosei/map.csv') as f:\n",
        "  reader = csv.reader(f)\n",
        "  for i, row in enumerate(reader):\n",
        "    if i == 0:\n",
        "      continue\n",
        "    l.append([row[1], row[2]])\n",
        "pprint.pprint(l)\n",
        "with open(f\"/content/drive/MyDrive/sosei/map_2kamoku.json\", 'w') as f:\n",
        "  json.dump(l, f, indent=2, ensure_ascii=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[['プログラミング', 'コンピュータグラフィクス'],\n",
            " ['プログラミング', 'プログラミングⅡ'],\n",
            " ['電気基礎', '電気基礎Ⅱ'],\n",
            " ['基礎数学F', 'メカニズム設計概論'],\n",
            " ['基礎数学E', 'ディジタル回路ⅠA'],\n",
            " ['力学', '力学Ⅱ'],\n",
            " ['線形代数A', 'エネルギー変換工学A'],\n",
            " ['線形代数A', 'エネルギー変換工学B'],\n",
            " ['線形代数A', '電磁気学Ⅰ'],\n",
            " ['線形代数B', 'コンピュータグラフィクス'],\n",
            " ['微分積分学ⅠA', '電磁気学Ⅰ'],\n",
            " ['微分積分学ⅠB', '電磁気学Ⅰ'],\n",
            " ['微分積分学ⅠB', '応用物理ⅠA'],\n",
            " ['力学Ⅱ', '電磁気学Ⅰ'],\n",
            " ['プログラミングⅡ', 'ロボットエレクトロニクス'],\n",
            " ['電気基礎Ⅱ', '電気回路ⅠA'],\n",
            " ['電気基礎Ⅱ', '電気回路ⅠB'],\n",
            " ['電気基礎Ⅱ', '電磁気学Ⅰ'],\n",
            " ['電気基礎Ⅱ', 'エネルギー変換工学A'],\n",
            " ['電気基礎Ⅱ', 'エネルギー変換工学B'],\n",
            " ['微分積分学ⅡB', '電磁気学Ⅰ'],\n",
            " ['応用物理ⅠA', 'メカニズム設計概論'],\n",
            " ['電気回路ⅠA', 'エネルギー変換工学A'],\n",
            " ['電気回路ⅠB', 'エネルギー変換工学A'],\n",
            " ['電気回路ⅠB', 'エネルギー変換工学B']]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d0jpAzzY_Z21"
      },
      "source": [
        "import json\n",
        "json_path = \"/content/drive/MyDrive/sosei/data1.json\"\n",
        "json_open = open(json_path, 'r')\n",
        "l = json.load(json_open)\n",
        "\n",
        "a = sorted(l, key=lambda x: x[2])\n",
        "output = []\n",
        "for i in range(30):\n",
        "  output.append(a[989-i])\n",
        "\n",
        "with open(f\"/content/drive/MyDrive/sosei/data1_top30.json\", 'w') as f:\n",
        "  json.dump(output, f, indent=2, ensure_ascii=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rAraD0pODo_Q"
      },
      "source": [
        "**2つのシラバスの全パターン**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I3W6eFxZDu8-"
      },
      "source": [
        "import glob\n",
        "from google import colab\n",
        "import itertools\n",
        "\n",
        "files = glob.glob(\"/content/drive/MyDrive/sosei/meishi/*\")\n",
        "print(len(files))\n",
        "kumiawase = list(itertools.combinations(files,2))\n",
        "print(len(kumiawase))\n",
        "pprint.pprint(kumiawase)\n",
        "for file1 in files:\n",
        "    print(file1)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GwGrKq7P_-ZC"
      },
      "source": [
        "# **Word2Vec**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AilqsOo3VajJ"
      },
      "source": [
        "東北大学の学習済みモデル"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PflNxc0RB4Lq",
        "outputId": "b71ed81e-ffaa-4cdf-bd8c-3869348a4097"
      },
      "source": [
        "#参考 https://gist.github.com/wai-doi/f0c85d64253b35746e3ca5fb327e10b8\n",
        "import gensim\n",
        "\n",
        "path = \"/content/drive/MyDrive/sosei/Pre-Trained Word2Vec Models/entity_vector/entity_vector.model.bin\"\n",
        "model = gensim.models.KeyedVectors.load_word2vec_format(path, binary=False)\n",
        "model.similarity('C言語', '男性')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.08955177"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nfMHZ557U7Qn"
      },
      "source": [
        "白ヤギコーポレーションの学習済みモデル\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 379
        },
        "id": "-MNeefQ5U0xx",
        "outputId": "9e174130-fca2-4eba-b1f3-4b0e343fc686"
      },
      "source": [
        "from gensim.models.word2vec import Word2Vec\n",
        "\n",
        "path = \"/content/drive/MyDrive/sosei/Pre-Trained Word2Vec Models/latest-ja-word2vec-gensim-model/word2vec.gensim.model\"\n",
        "w2v_model = Word2Vec.load(path)\n",
        "try:\n",
        "  print(w2v_model.similarity('C言語', 'C言語'))\n",
        "  print(w2v_model.similarity('c言語', 'Python'))\n",
        "  print(w2v_model.similarity('八戸', '八戸'))\n",
        "except KeyError:\n",
        "  print(-1)\n",
        "\n",
        "print(w2v_model.similarity('八戸', '八戸'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-38c8302dfa26>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"/content/drive/MyDrive/sosei/Pre-Trained Word2Vec Models/latest-ja-word2vec-gensim-model/word2vec.gensim.model\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mw2v_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWord2Vec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw2v_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'東京'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gensim/models/word2vec.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1310\u001b[0m         \"\"\"\n\u001b[1;32m   1311\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1312\u001b[0;31m             \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mWord2Vec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1313\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1314\u001b[0m             \u001b[0;31m# for backward compatibility for `max_final_vocab` feature\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gensim/models/base_any2vec.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1242\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1243\u001b[0m         \"\"\"\n\u001b[0;32m-> 1244\u001b[0;31m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBaseWordEmbeddingsModel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1245\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ns_exponent'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1246\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mns_exponent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.75\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gensim/models/base_any2vec.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(cls, fname_or_handle, **kwargs)\u001b[0m\n\u001b[1;32m    601\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    602\u001b[0m         \"\"\"\n\u001b[0;32m--> 603\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBaseAny2VecModel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname_or_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    604\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    605\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname_or_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gensim/utils.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(cls, fname, mmap)\u001b[0m\n\u001b[1;32m    420\u001b[0m         \u001b[0mcompress\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSaveLoad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_adapt_by_suffix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 422\u001b[0;31m         \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munpickle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    423\u001b[0m         \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_load_specials\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmmap\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompress\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    424\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"loaded %s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gensim/utils.py\u001b[0m in \u001b[0;36munpickle\u001b[0;34m(fname)\u001b[0m\n\u001b[1;32m   1356\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1357\u001b[0m     \"\"\"\n\u001b[0;32m-> 1358\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0msmart_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1359\u001b[0m         \u001b[0;31m# Because of loading from S3 load can't be used (missing readline in smart_open)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1360\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversion_info\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/smart_open/smart_open_lib.py\u001b[0m in \u001b[0;36msmart_open\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    496\u001b[0m     \u001b[0mignore_ext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mignore_extension\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    497\u001b[0m     \u001b[0;32mdel\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_extension\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 498\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mlocals\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    499\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    500\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/smart_open/smart_open_lib.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(uri, mode, buffering, encoding, errors, newline, closefd, opener, ignore_ext, compression, transport_params)\u001b[0m\n\u001b[1;32m    193\u001b[0m         \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m         \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 195\u001b[0;31m         \u001b[0mnewline\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnewline\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m     )\n\u001b[1;32m    197\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfobj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/smart_open/smart_open_lib.py\u001b[0m in \u001b[0;36m_shortcut_open\u001b[0;34m(uri, mode, compression, buffering, encoding, errors, newline)\u001b[0m\n\u001b[1;32m    359\u001b[0m         \u001b[0mopen_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'errors'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    360\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 361\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_builtin_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocal_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffering\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbuffering\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mopen_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    362\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    363\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/MyDrive/sosei/Pre-Trained Word2Vec Models/latest-ja-word2vec-gensim-model/word2vec.gensim.model'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n1SWGSsWXH8a"
      },
      "source": [
        "可視化"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p0jX9bx1XLQn",
        "outputId": "4aa68437-e129-41f5-9b7d-73c543ee05cc"
      },
      "source": [
        "# plain word2vec t-SNE Visualization\n",
        "from gensim.models.word2vec import Word2Vec\n",
        "import numpy as np\n",
        "from sklearn.manifold import TSNE\n",
        "\n",
        "path = \"/content/drive/MyDrive/sosei/latest-ja-word2vec-gensim-model/word2vec.gensim.model\"\n",
        "model = Word2Vec.load(path)\n",
        "word2vec_model=model\n",
        "\n",
        "skip=0\n",
        "limit=241 \n",
        "\n",
        "vocab = word2vec_model.wv.vocab\n",
        "emb_tuple = tuple([word2vec_model[v] for v in vocab])\n",
        "X = np.vstack(emb_tuple)\n",
        "\n",
        "tsne_model = TSNE(n_components=2, random_state=0,verbose=2)\n",
        "np.set_printoptions(suppress=True)\n",
        "tsne_model.fit_transform(X)\n",
        "\n",
        "plain_tsne = pd.DataFrame(tsne_model.embedding_[skip:limit, 0],columns = [\"x\"])\n",
        "plain_tsne[\"y\"] = pd.DataFrame(tsne_model.embedding_[skip:limit, 1])\n",
        "plain_tsne[\"word\"] = list(vocab)[skip:limit]\n",
        "# plain_tsne[\"cluster\"] = idx[skip:limit] # クラスタを計算し終わったあとならここでクラスタを付与できます\n",
        "# plain_tsne.plot.scatter(x=\"x\",y=\"y\",c=\"cluster\",cmap=\"viridis\",figsize=(8, 6),s=30)\n",
        "plain_tsne.plot.scatter(x=\"x\",y=\"y\",figsize=(8, 6),s=30)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:14: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
            "  \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[t-SNE] Computing 91 nearest neighbors...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H4e-jTJY_Z8c"
      },
      "source": [
        "# **LDA**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WQmBQr6QgkD5"
      },
      "source": [
        "# MeCabオブジェクトの生成\n",
        "mt = MeCab.Tagger('')\n",
        "mt.parse('')\n",
        "\n",
        "# トピック数の設定\n",
        "NUM_TOPICS = 3\n",
        "\n",
        "# トレーニングデータの読み込み\n",
        "# train_texts は二次元のリスト\n",
        "# テキストデータを一件ずつ分かち書き（名詞、動詞、形容詞に限定）して train_texts に格納するだけ\n",
        "train_texts = []\n",
        "f = [\n",
        "      \"日本防衛 長射程重視に変質も\",\n",
        "      \"合区で活用 参院特定枠って\",\n",
        "      \"21議員の政治資金 使途不透明\",\n",
        "      \"公害で死んだ娘 語り継ぐ教訓\",\n",
        "      \"「隠れた被害者」加害者の子\",\n",
        "      \"ふるさと納税 寄付が広告費に\",\n",
        "      \"全国に外国人相談窓口 通訳も\",\n",
        "      \"ぐずりにスマホ 非難に悩む親\"\n",
        "]\n",
        "for line in f:\n",
        "  text = []\n",
        "  node = mt.parseToNode(line.strip())\n",
        "  while node:\n",
        "      fields = node.feature.split(\",\")\n",
        "      if fields[0] == '名詞' or fields[0] == '動詞' or fields[0] == '形容詞':\n",
        "          text.append(node.surface)\n",
        "      node = node.next\n",
        "  train_texts.append(text)\n",
        "\n",
        "# モデル作成\n",
        "dictionary = Dictionary(train_texts)\n",
        "corpus = [dictionary.doc2bow(text) for text in train_texts]\n",
        "lda = LdaModel(corpus=corpus, num_topics=NUM_TOPICS, id2word=dictionary)\n",
        "\n",
        "# テストデータ読み込み\n",
        "# test_texts は train_texts と同じフォーマット\n",
        "test_texts = []\n",
        "raw_test_texts = []\n",
        "f = [\n",
        "    \"爆発 現場で缶100本ガス抜き\",\n",
        "    \"爆発の瞬間 デマ動画が拡散\",\n",
        "    \"バニラエア 来年10月運航終了\",\n",
        "    \"コンビニごみ 店舗負担の矛盾\",\n",
        "    \"謎めいたカエル 南米で再発見\",\n",
        "    \"イニエスタ うつ報道の難しさ\",\n",
        "    \"ムネリン笑顔「少し元気に」\",\n",
        "    \"G菅野 ゴジラ超え6.5億円に\"\n",
        "]\n",
        "for line in f:\n",
        "    text = []\n",
        "    raw_test_texts.append(line.strip())\n",
        "    node = mt.parseToNode(line.strip())\n",
        "    while node:\n",
        "        fields = node.feature.split(\",\")\n",
        "        if fields[0] == '名詞' or fields[0] == '動詞' or fields[0] == '形容詞':\n",
        "            text.append(node.surface)\n",
        "        node = node.next\n",
        "    test_texts.append(text)\n",
        "\n",
        "\n",
        "# テストデータをモデルに掛ける\n",
        "score_by_topic = defaultdict(int)\n",
        "test_corpus = [dictionary.doc2bow(text) for text in test_texts]\n",
        "\n",
        "# クラスタリング結果を出力\n",
        "for unseen_doc, raw_train_text in zip(test_corpus, raw_test_texts):\n",
        "    print(raw_train_text, end='\\t')\n",
        "    for topic, score in lda[unseen_doc]:\n",
        "        score_by_topic[int(topic)] = float(score)\n",
        "    for i in range(NUM_TOPICS):\n",
        "        print('{:.2f}'.format(score_by_topic[i]), end='\\t')\n",
        "    print()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zppn40YdTXyV"
      },
      "source": [
        "# **Doc2Vec**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fh9v_1JbTbjX",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 359
        },
        "outputId": "13fc7b85-a884-4316-d50a-1e4ab31fe99b"
      },
      "source": [
        "#参考 https://yag-ays.github.io/project/pretrained_doc2vec_wikipedia/\n",
        "\n",
        "from gensim.models.doc2vec import Doc2Vec\n",
        "\n",
        "model_path = \"/content/drive/MyDrive/sosei/Pre-Trained Doc2Vec Models/jawiki.doc2vec.dbow300d/jawiki.doc2vec.dbow300d.model\"\n",
        "doc1_path = \"/content/drive/MyDrive/sosei/bunsho/mono_zukuri_kiso.txt\"\n",
        "doc2_path = \"/content/drive/MyDrive/sosei/bunsho/puroguraming_1.txt\"\n",
        "model = Doc2Vec.load(model_path)\n",
        "model.docvecs.similarity(doc1_path, doc2_path)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-56137476eebb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mdoc2_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"/content/drive/MyDrive/sosei/bunsho/puroguraming_1.txt\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDoc2Vec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdocvecs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msimilarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc1_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdoc2_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36msimilarity\u001b[0;34m(self, d1, d2)\u001b[0m\n\u001b[1;32m   1717\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1718\u001b[0m         \"\"\"\n\u001b[0;32m-> 1719\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmatutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munitvec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0md1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmatutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munitvec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0md2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1720\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1721\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mn_similarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mds1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mds2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m   1529\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvectors_docs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_int_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdoctags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_rawint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1530\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1531\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"tag '%s' not seen in training corpus/invalid\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1532\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1533\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__contains__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: \"tag '/content/drive/MyDrive/sosei/bunsho/mono_zukuri_kiso.txt' not seen in training corpus/invalid\""
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hyCmtEBs_pnO"
      },
      "source": [
        "# **Google Drive マウント**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c9FoO-ZhSJIW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "26f8e620-ee9b-4b4b-b21b-0e89773f57d3"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}